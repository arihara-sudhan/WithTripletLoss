{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cc2974",
   "metadata": {},
   "source": [
    "# ‚≠ê DOCUMENT DETECTION - TRIPLET LOSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24afab85",
   "metadata": {},
   "source": [
    "# ![https://a-z-animals.com/media/2021/10/sumatran-tiger-panthera-tigris-sumatrae-cub-standing-on-rock-picture-id1254523938.jpg](https://a-z-animals.com/media/2021/10/sumatran-tiger-panthera-tigris-sumatrae-cub-standing-on-rock-picture-id1254523938.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2ca688",
   "metadata": {},
   "source": [
    "# IMPORT REQUIRED LIBRARIES üåß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7279d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam,lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.nn.parallel import DataParallel\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import os, pickle, random, time, cv2, faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafad6b",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3c6ac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0930f849",
   "metadata": {},
   "source": [
    "# CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b73c9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self._load_images()\n",
    "\n",
    "    def _load_images(self):\n",
    "        valid_extensions = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "        for class_name in os.listdir(self.folder_path):\n",
    "            class_folder = os.path.join(self.folder_path, class_name)\n",
    "            if os.path.isdir(class_folder):\n",
    "                for filename in os.listdir(class_folder):\n",
    "                    if filename.lower().endswith(valid_extensions):\n",
    "                        self.image_paths.append(os.path.join(class_folder, filename))\n",
    "                        self.labels.append(class_name)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e904d2",
   "metadata": {},
   "source": [
    "# CONCATENATING DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0701557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatenatedDataset(Dataset):\n",
    "    def __init__(self, dataset_list, transform=None):\n",
    "        self.dataset_list = dataset_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(dataset) for dataset in self.dataset_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for dataset in self.dataset_list:\n",
    "            if idx < len(dataset):\n",
    "                return dataset[idx]\n",
    "            idx -= len(dataset)\n",
    "    \n",
    "    def _transform_image(self, image):\n",
    "        if self.transform:\n",
    "            return self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def _load_image(self, image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return self._transform_image(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87030d2f",
   "metadata": {},
   "source": [
    "# TRAIN AND TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f5b6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = [\n",
    "                 \"./Datasets/TOBACCO/train/\",\n",
    "                 \"./Datasets/ARABIC_DOCS/train/\",\n",
    "                 \"./Datasets/DOCS/train/\",\n",
    "                 \"./Datasets/LANG_DOCS/train/\",\n",
    "                 \"./Datasets/MLIMGS/train/\"\n",
    "]\n",
    "\n",
    "test_folders = [\n",
    "                 \"./Datasets/TOBACCO/test/\",\n",
    "                 \"./Datasets/ARABIC_DOCS/test/\",\n",
    "                 \"./Datasets/DOCS/test/\",\n",
    "                 \"./Datasets/LANG_DOCS/test/\",\n",
    "                 \"./Datasets/MLIMGS/test/\"\n",
    "]\n",
    "train_datasets = [CustomDataset(folder, transform=transform) for folder in train_folders]\n",
    "test_datasets = [CustomDataset(folder, transform=transform) for folder in test_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6b127841",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_dataset = ConcatenatedDataset(train_datasets)\n",
    "combined_test_dataset = ConcatenatedDataset(test_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35cb04",
   "metadata": {},
   "source": [
    "# TRIPLETIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "59588308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.num_samples = len(self.base_dataset)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor, anchor_label = self.base_dataset[idx]\n",
    "        positive_idx = idx\n",
    "        while positive_idx == idx:\n",
    "            positive_idx = random.randint(0, self.num_samples - 1)\n",
    "        positive, positive_label = self.base_dataset[positive_idx]\n",
    "\n",
    "        negative_idx = idx\n",
    "        while negative_idx == idx or self.base_dataset[negative_idx][1] == anchor_label:\n",
    "            negative_idx = random.randint(0, self.num_samples - 1)\n",
    "        negative, negative_label = self.base_dataset[negative_idx]\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "train_set = ConcatenatedDataset(train_datasets)\n",
    "test_set = ConcatenatedDataset(test_datasets)\n",
    "\n",
    "train_set = TripletDataset(train_set)\n",
    "test_set = TripletDataset(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f4d73e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31549, 3460)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1ff88",
   "metadata": {},
   "source": [
    "# LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4ad6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733eab2",
   "metadata": {},
   "source": [
    "# THE MODEL ‚ú®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a748a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        self.convnet = nn.Sequential(*list(resnet50.children())[:-1])  # Remove the fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9dc52a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8081405",
   "metadata": {},
   "source": [
    "# TRIPLET WRAPPER ‚òòÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89129096",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if x2 is None and x3 is None:\n",
    "            return self.embedding_net(x1)\n",
    "        return self.embedding_net(x1),self.embedding_net(x2),self.embedding_net(x3)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7e68c0",
   "metadata": {},
   "source": [
    "# TRIPLET LOSS üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "644c813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = torch.norm(anchor - positive, dim=1)\n",
    "        distance_negative = torch.norm(anchor - negative, dim=1)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31a4e96",
   "metadata": {},
   "source": [
    "# SET THE STUFFS UP  üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b4433101",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TripletNet(emb)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "margin = 1\n",
    "lr = 0.0001\n",
    "#n_epochs = int(input(\"NO OF EPOCHS : \"))\n",
    "n_epochs = 2\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "loss_fn = TripletLoss(margin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c56fc68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = './Models/samplervl50.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2454f0",
   "metadata": {},
   "source": [
    "# EVALUATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4891d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, triplet_test_loader,for_log=False,LIMIT=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for idx,(anchor, positive, negative) in enumerate(triplet_test_loader):\n",
    "            if for_log and idx==LIMIT:\n",
    "                return f'ACCURACY: {correct/total*100}% ,TIME: {time.time()-start}'\n",
    "            anchor_embedding, positive_embedding, negative_embedding = model(anchor.to(device),\n",
    "                                                                             positive.to(device),\n",
    "                                                                             negative.to(device))\n",
    "            distance_positive = torch.norm(anchor_embedding - positive_embedding, dim=1).to(device)\n",
    "            distance_negative = torch.norm(anchor_embedding - negative_embedding, dim=1).to(device)\n",
    "            correct += torch.sum(distance_positive < distance_negative).item()\n",
    "            total += anchor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(accuracy*100,time.time()-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16ace4",
   "metadata": {},
   "source": [
    "# TRAIN ü¶ú"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c75d29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, num_epochs, train_loader,bs):\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            anchor, positive, negative = batch\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            anchor_embedding, positive_embedding, negative_embedding = model(anchor, positive, negative)\n",
    "            anchor_embedding.requires_grad_(True)\n",
    "            positive_embedding.requires_grad_(True)\n",
    "            negative_embedding.requires_grad_(True)\n",
    "            loss = loss_fn(anchor_embedding, positive_embedding, negative_embedding)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            print(f\"({idx + 1}).  LOSS : {loss.item()}  SEEN : {bs * (idx + 1)}/{len(train_loader.dataset)}\")\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss / len(train_loader):.4f}, TIME: {time.time()-start}\")\n",
    "        #print('VALIDATION :')\n",
    "        #evaluate_model(model, valid_loader)\n",
    "        print('TESTING :')\n",
    "        evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc411d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit(model,n_epochs,train_loader,bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "092552e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL SAVED!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model,\"./Models/PANTHER.pt\")\n",
    "print(\"MODEL SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc5e6ee",
   "metadata": {},
   "source": [
    "# WITH FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a94b4b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folders = [\n",
    "                 \"./Datasets/TOBACCO/train/\",\n",
    "                 \"./Datasets/ARABIC_DOCS/train/\",\n",
    "                 \"./Datasets/DOCS/train/\",\n",
    "                 \"./Datasets/LANG_DOCS/train/\",\n",
    "                 \"./Datasets/MLIMGS/train/\"\n",
    "]\n",
    "\n",
    "test_folders = [\n",
    "                 \"./Datasets/TOBACCO/test/\",\n",
    "                 \"./Datasets/ARABIC_DOCS/test/\",\n",
    "                 \"./Datasets/DOCS/test/\",\n",
    "                 \"./Datasets/LANG_DOCS/test/\",\n",
    "                 \"./Datasets/MLIMGS/test/\"\n",
    "]\n",
    "train_datasets = [CustomDataset(folder, transform=transform) for folder in train_folders]\n",
    "test_datasets = [CustomDataset(folder, transform=transform) for folder in test_folders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2f18b161",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ConcatenatedDataset(train_datasets)\n",
    "test_set = ConcatenatedDataset(test_datasets)\n",
    "train_dataloader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "test_dataloader = DataLoader(test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421ae83b",
   "metadata": {},
   "source": [
    "# EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8fe70707",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m train_embs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(idx)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 521\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    524\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    560\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    563\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 44\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[62], line 12\u001b[0m, in \u001b[0;36mConcatenatedDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_list:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset):\n\u001b[0;32m---> 12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     13\u001b[0m     idx \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "Cell \u001b[0;32mIn[60], line 27\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m     25\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 27\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     29\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "File \u001b[0;32m~/miniconda3/envs/arisenv/lib/python3.8/site-packages/PIL/Image.py:911\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    865\u001b[0m ):\n\u001b[1;32m    866\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    913\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/arisenv/lib/python3.8/site-packages/PIL/ImageFile.py:269\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    268\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 269\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_embs = None\n",
    "train_labels = []\n",
    "for idx,i in enumerate(train_dataloader):\n",
    "    if idx%100==0:\n",
    "        print(idx)\n",
    "    I, L = i\n",
    "    train_labels.append(L)\n",
    "    emb = model(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if train_embs is None:\n",
    "        train_embs = emb\n",
    "    else:\n",
    "        train_embs = torch.cat((train_embs, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "efd5a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "test_embs = None\n",
    "test_labels = []\n",
    "for idx,i in enumerate(test_dataloader):\n",
    "    if idx%100==0:\n",
    "        print(idx)\n",
    "    I, L = i\n",
    "    test_labels.append(L)\n",
    "    emb = model(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if test_embs is None:\n",
    "        test_embs = emb\n",
    "    else:\n",
    "        test_embs = torch.cat((test_embs, emb), dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9c058a",
   "metadata": {},
   "source": [
    "# DIFFERENT INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "151c43ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 1968 points to 100 centroids: please provide at least 3900 training points\n"
     ]
    }
   ],
   "source": [
    "embs_cpu_np = train_embs.cpu().numpy()\n",
    "embs_cpu_np = embs_cpu_np.reshape(embs_cpu_np.shape[0], -1)\n",
    "\n",
    "index1 = faiss.IndexFlatL2(embs_cpu_np.shape[1])  # Assuming embs_cpu_np.shape[1] represents the dimensionality of the embeddings\n",
    "index1.add(embs_cpu_np)\n",
    "\n",
    "nlist = 100  # Number of cells/buckets\n",
    "\n",
    "quantizer = faiss.IndexFlatL2(embs_cpu_np.shape[1])  # Quantizer index (same as IndexFlatL2)\n",
    "index2 = faiss.IndexIVFFlat(quantizer, embs_cpu_np.shape[1], nlist)\n",
    "index2.train(embs_cpu_np)\n",
    "index2.add(embs_cpu_np)\n",
    "\n",
    "index3 = faiss.IndexHNSWFlat(embs_cpu_np.shape[1], 128)  # M = 32 for the HNSW index\n",
    "index3.add(embs_cpu_np)\n",
    "\n",
    "nbits = 8  # Number of bits for the LSH hash\n",
    "index4 = faiss.IndexLSH(embs_cpu_np.shape[1], nbits)\n",
    "index4.add(embs_cpu_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8aa18dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatewithfaiss(embs,index):\n",
    "    TOTAL = len(embs)\n",
    "    CORRECT = 0\n",
    "    start = time.time()\n",
    "    for idx,emb in enumerate(embs):\n",
    "        label = index.search(emb.reshape(1,-1),1)[1][0][0]\n",
    "        if train_labels[label][0]==test_labels[idx][0]:\n",
    "            CORRECT += 1\n",
    "    return f'{CORRECT}/{TOTAL}={(CORRECT/TOTAL)*100}',f'TIME = {time.time()-start} SECONDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "49de7f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs2_cpu_np = test_embs.cpu().numpy()\n",
    "embs2_cpu_np = embs2_cpu_np.reshape(embs2_cpu_np.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "850663c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexIVFFlat : ('359/398=90.20100502512562', 'TIME = 0.07941412925720215 SECONDS')\n",
      "IndexHNSWFlat : ('358/398=89.9497487437186', 'TIME = 0.25783753395080566 SECONDS')\n",
      "IndexLSH : ('217/398=54.52261306532663', 'TIME = 0.012828350067138672 SECONDS')\n",
      "IndexFlatL2 : ('358/398=89.9497487437186', 'TIME = 1.2119579315185547 SECONDS')\n"
     ]
    }
   ],
   "source": [
    "print(f'IndexIVFFlat : {evaluatewithfaiss(embs2_cpu_np,index2)}')\n",
    "print(f'IndexHNSWFlat : {evaluatewithfaiss(embs2_cpu_np,index3)}')\n",
    "print(f'IndexLSH : {evaluatewithfaiss(embs2_cpu_np,index4)}')\n",
    "print(f'IndexFlatL2 : {evaluatewithfaiss(embs2_cpu_np,index1)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
