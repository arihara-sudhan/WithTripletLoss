{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ae10da1",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03796efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torch.nn.functional import normalize\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from torchvision import transforms as T,transforms\n",
    "import time\n",
    "import torch.nn.functional as func\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1883c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ac8f91b",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "113d5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='google/vit-base-patch16-224-in21k'\n",
    "extractor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "341e8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.feature_extractor = extractor\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = (x + 1) / 2\n",
    "        inputs = self.feature_extractor(images=x, return_tensors='pt', do_normalize=True, image_mean=0.5, image_std=0.5)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs).last_hidden_state[:, 0].cpu().requires_grad_()\n",
    "        return outputs\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "        \n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if x2 is None and x3 is None:\n",
    "            return self.embedding_net(x1)\n",
    "        return self.embedding_net(x1),self.embedding_net(x2),self.embedding_net(x3)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb32c8",
   "metadata": {},
   "source": [
    "# CUSTOMIZE MNIST INTO TRIPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f26af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMNISTDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        anchor = self.data[index]\n",
    "        anchor_target = self.targets[index]\n",
    "        positive_indices = torch.where(self.targets == anchor_target)[0]\n",
    "        positive_indices = positive_indices[positive_indices != index]\n",
    "        positive_index = torch.randint(0, len(positive_indices), (1,))\n",
    "        positive = self.data[positive_indices[positive_index]]\n",
    "\n",
    "        negative_indices = torch.where(self.targets != anchor_target)[0]\n",
    "        negative_index = torch.randint(0, len(negative_indices), (1,))\n",
    "        negative = self.data[negative_indices[negative_index]]\n",
    "\n",
    "        return anchor, positive, negative\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = MNIST(root=\"./Datasets/MNIST\", train=True, download=True)\n",
    "train_ratio = 0.9\n",
    "val_ratio = 1 - train_ratio\n",
    "train_size = int(train_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "test_dataset = MNIST(root=\"./Datasets/MNIST\", train=False, download=True)\n",
    "train_triplet_dataset = TripletMNISTDataset(train_dataset.dataset.data, train_dataset.dataset.targets)\n",
    "val_triplet_dataset = TripletMNISTDataset(val_dataset.dataset.data, train_dataset.dataset.targets)\n",
    "test_triplet_dataset = TripletMNISTDataset(test_dataset.data, test_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df2b6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece79b62",
   "metadata": {},
   "source": [
    "# MODEL SPECIFIC TRANSFORMATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a09dbd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation_chain = T.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "        transforms.Lambda(lambda x: x.convert(\"RGB\")),  # Convert grayscale to RGB\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std),\n",
    "    ]\n",
    "\n",
    ")\n",
    "\n",
    "def collate(batch):\n",
    "    anchor_transformed = [transformation_chain(img) for img in batch[0]]\n",
    "    pos_transformed = [transformation_chain(img) for img in batch[1]]\n",
    "    neg_transformed = [transformation_chain(img) for img in batch[2]]\n",
    "    anchor_tensors = torch.stack(anchor_transformed)\n",
    "    positive_tensors = torch.stack(pos_transformed)\n",
    "    negative_tensors = torch.stack(neg_transformed)\n",
    "    return anchor_tensors, positive_tensors, negative_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191dc382",
   "metadata": {},
   "source": [
    "# TRIPLET LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a1191b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = func.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297acf4",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3794673",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TripletNet(\n",
       "  (embedding_net): EmbeddingNet(\n",
       "    (model): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): ViTPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 1280\n",
    "train_loader = DataLoader(train_triplet_dataset, batch_size=bs,collate_fn=collate)\n",
    "val_loader = DataLoader(val_triplet_dataset, batch_size=bs,collate_fn=collate)\n",
    "triplet_loss = TripletLoss(margin=0.2)\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = EmbeddingNet()\n",
    "model = TripletNet(model1)\n",
    "model.requires_grad_(True)\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71395593",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d630264",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for idx,batch in enumerate(train_loader):\n",
    "        anchor, positive, negative = batch\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        anchor_embedding, positive_embedding, negative_embedding = model(anchor,positive,negative)\n",
    "        anchor_embedding.requires_grad_(True)\n",
    "        positive_embedding.requires_grad_(True)\n",
    "        negative_embedding.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        loss = triplet_loss(anchor_embedding, positive_embedding, negative_embedding)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if idx%10==0:\n",
    "            print(f'({idx}).  LOSS : {loss.item()}  SEEN : {bs*(idx+1)}/{60000}')\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
    "    evaluate_model(model,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "17e701ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'Models/tripletMNISTrans.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79f132b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load('Models/tripletMNISTrans.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee71c60",
   "metadata": {},
   "source": [
    "# EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2364f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, triplet_test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for (anchor, positive, negative) in triplet_test_loader:\n",
    "            anchor_embedding, positive_embedding, negative_embedding = model(anchor, positive, negative)\n",
    "            positive_distance = func.pairwise_distance(anchor_embedding, positive_embedding)\n",
    "            negative_distance = func.pairwise_distance(anchor_embedding, negative_embedding)\n",
    "            correct += torch.sum(positive_distance < negative_distance).item()\n",
    "            total += anchor.size(0)  # Increment by the batch size\n",
    "    accuracy = correct / total\n",
    "    print('Validation Set Accuracy: {:.2f}% |||| Time: {:.2f} SECONDS'.format(accuracy * 100, time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "5e51ca2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 33.33%\n",
      "Time: 4.11 SECONDS\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(test_triplet_dataset, batch_size=5000, collate_fn=collate)\n",
    "evaluate_model(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2431e788",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 66.67%\n",
      "Time : 37.82 SECONDS\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model,train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b0ccd",
   "metadata": {},
   "source": [
    "# WITH FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "bbce3668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "5021bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_(batch):\n",
    "    return torch.stack([transformation_chain(img) for img in batch[0][0]]),batch[0][1]\n",
    "\n",
    "test_loader_ = torch.utils.data.DataLoader(test_dataset, shuffle=True,collate_fn=collate_)\n",
    "train_loader_ = torch.utils.data.DataLoader(train_dataset, shuffle=True,collate_fn = collate_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0371edc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "embs1 = None\n",
    "labels1 = []\n",
    "for idx,i in enumerate(train_loader_):\n",
    "    if idx==100: break\n",
    "    I, L = i\n",
    "    labels1.append(L)\n",
    "    print(idx)\n",
    "    emb = model_loaded(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if embs1 is None:\n",
    "        embs1 = emb\n",
    "    else:\n",
    "        embs1 = torch.cat((embs1, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "9b621b6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "embs2 = None\n",
    "labels2 = []\n",
    "for idx,i in enumerate(test_loader_):\n",
    "    if idx==100: break\n",
    "    I, L = i\n",
    "    labels2.append(L)\n",
    "    print(idx)\n",
    "    emb = model_loaded(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if embs2 is None:\n",
    "        embs2 = emb\n",
    "    else:\n",
    "        embs2 = torch.cat((embs2, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fab629",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "5b6c9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING clustering 100 points to 100 centroids: please provide at least 3900 training points\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "embs = embs2\n",
    "\n",
    "index1 = faiss.IndexFlatL2(embs.shape[1])  # Assuming embs.shape[1] represents the dimensionality of the embeddings\n",
    "index1.add(embs)\n",
    "\n",
    "nlist = 100  # Number of cells/buckets\n",
    "quantizer = faiss.IndexFlatL2(embs.shape[1])  # Quantizer index (same as IndexFlatL2)\n",
    "index2 = faiss.IndexIVFFlat(quantizer, embs.shape[1], nlist)\n",
    "index2.train(embs)\n",
    "index2.add(embs)\n",
    "\n",
    "index3 = faiss.IndexHNSWFlat(embs.shape[1], 32)  # M = 32 for the HNSW index\n",
    "index3.add(embs)\n",
    "\n",
    "nbits = 8  # Number of bits for the LSH hash\n",
    "index4 = faiss.IndexLSH(embs.shape[1], nbits)\n",
    "index4.add(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "55e4fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2[index1.search(embs1[0].detach().reshape(1,-1),1)[1][0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fcba9143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatewithfaiss(embs,index):\n",
    "    TOTAL = len(embs)\n",
    "    CORRECT = 0\n",
    "    start = time.time()\n",
    "    for idx,emb in enumerate(embs):\n",
    "        label = index.search(emb.detach().reshape(1,-1),2)[1][0][0]\n",
    "        CORRECT += labels1[label]==labels1[idx]\n",
    "    return (CORRECT/TOTAL*100),f'{time.time()-start} SECONDS'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e140eba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'IndexFlatL2 : {evaluatewithfaiss(embs2,index1)}')\n",
    "print(f'IndexIVFFlat : {evaluatewithfaiss(embs2,index2)}')\n",
    "print(f'IndexHNSWFlat : {evaluatewithfaiss(embs2,index3)}')\n",
    "print(f'IndexLSH : {evaluatewithfaiss(embs2,index4)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d91337f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "evaluate_model(model,triplet_train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
