{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b437f004",
   "metadata": {},
   "source": [
    "# IMPORT REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60608e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526a320",
   "metadata": {},
   "source": [
    "# LOAD DATASET AND NORMALIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4eb484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = 0.1307, 0.3081\n",
    "\n",
    "train_dataset = MNIST('./Datasets/MNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = MNIST('./Datasets/MNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d40c72",
   "metadata": {},
   "source": [
    "# TRIPLET LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5762b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58ee8b",
   "metadata": {},
   "source": [
    "# CUSTOMIZE MNIST TO BE TRIPLETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeacde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "        else:\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            # generate fixed triplets for testing\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            triplets = [[i,\n",
    "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                         random_state.choice(self.label_to_indices[\n",
    "                                                 np.random.choice(\n",
    "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                 )\n",
    "                                             ])\n",
    "                         ]\n",
    "                        for i in range(len(self.test_data))]\n",
    "            self.test_triplets = triplets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "            img2 = self.train_data[positive_index]\n",
    "            img3 = self.train_data[negative_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_triplets[index][0]]\n",
    "            img2 = self.test_data[self.test_triplets[index][1]]\n",
    "            img3 = self.test_data[self.test_triplets[index][2]]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        img3 = Image.fromarray(img3.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        return (img1, img2, img3), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05706390",
   "metadata": {},
   "source": [
    "# NEWORK ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "134146b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 2)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b4c92",
   "metadata": {},
   "source": [
    "# TRIPLET WRAPPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7324977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if x2 is None and x3 is None:\n",
    "            return self.embedding_net(x1)\n",
    "        return self.embedding_net(x1),self.embedding_net(x2),self.embedding_net(x3)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e7686",
   "metadata": {},
   "source": [
    "# SETUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c04d93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ari-18308/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/ari-18308/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/ari-18308/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/home/ari-18308/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "triplet_train_dataset = TripletMNIST(train_dataset) \n",
    "triplet_test_dataset = TripletMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  shuffle=False,\n",
    "                                                  **kwargs)\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0488fef",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be26f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n",
    "        start_epoch=0):\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        scheduler.step()\n",
    "        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target if len(target) > 0 else None\n",
    "        if not type(data) in (tuple, list):\n",
    "            data = (data,)\n",
    "        if cuda:\n",
    "            data = tuple(d.cuda() for d in data)\n",
    "            if target is not None:\n",
    "                target = target.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*data)\n",
    "\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "\n",
    "        loss_inputs = outputs\n",
    "        if target is not None:\n",
    "            target = (target,)\n",
    "            loss_inputs += target\n",
    "\n",
    "        loss_outputs = loss_fn(*loss_inputs)\n",
    "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(outputs, target, loss_outputs)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message)\n",
    "            losses = []\n",
    "\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4b1f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/60000 (0%)]\tLoss: 0.254117\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.236703\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.139143\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.106591\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.072952\n",
      "Epoch: 1/20. Train set: Average loss: 0.1284\n",
      "Train: [0/60000 (0%)]\tLoss: 0.072379\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.054103\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.052839\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.048049\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.044118\n",
      "Epoch: 2/20. Train set: Average loss: 0.0508\n",
      "Train: [0/60000 (0%)]\tLoss: 0.077709\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.037936\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.025283\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.031766\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.035061\n",
      "Epoch: 3/20. Train set: Average loss: 0.0315\n",
      "Train: [0/60000 (0%)]\tLoss: 0.007449\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.022308\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.030332\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.022173\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.023383\n",
      "Epoch: 4/20. Train set: Average loss: 0.0239\n",
      "Train: [0/60000 (0%)]\tLoss: 0.002698\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.022861\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.025285\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.021144\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.020815\n",
      "Epoch: 5/20. Train set: Average loss: 0.0221\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.015986\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.013339\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.016748\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.017475\n",
      "Epoch: 6/20. Train set: Average loss: 0.0166\n",
      "Train: [0/60000 (0%)]\tLoss: 0.008227\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.010792\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.009835\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.008830\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.006412\n",
      "Epoch: 7/20. Train set: Average loss: 0.0088\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.006211\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.007445\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.005668\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003798\n",
      "Epoch: 8/20. Train set: Average loss: 0.0057\n",
      "Train: [0/60000 (0%)]\tLoss: 0.008818\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.005976\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.004483\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.004395\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.004911\n",
      "Epoch: 9/20. Train set: Average loss: 0.0049\n",
      "Train: [0/60000 (0%)]\tLoss: 0.005020\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.004640\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003532\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003305\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.005155\n",
      "Epoch: 10/20. Train set: Average loss: 0.0043\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003107\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003350\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002897\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.004442\n",
      "Epoch: 11/20. Train set: Average loss: 0.0033\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003974\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003315\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003256\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.004119\n",
      "Epoch: 12/20. Train set: Average loss: 0.0035\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.004273\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003631\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003684\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003180\n",
      "Epoch: 13/20. Train set: Average loss: 0.0035\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003440\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002049\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002682\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003764\n",
      "Epoch: 14/20. Train set: Average loss: 0.0027\n",
      "Train: [0/60000 (0%)]\tLoss: 0.001763\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.002514\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002592\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003058\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002232\n",
      "Epoch: 15/20. Train set: Average loss: 0.0024\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.002325\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.001949\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002322\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.001612\n",
      "Epoch: 16/20. Train set: Average loss: 0.0021\n",
      "Train: [0/60000 (0%)]\tLoss: 0.042917\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.001515\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002647\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002850\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002189\n",
      "Epoch: 17/20. Train set: Average loss: 0.0022\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.001708\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002823\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.001361\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.001631\n",
      "Epoch: 18/20. Train set: Average loss: 0.0019\n",
      "Train: [0/60000 (0%)]\tLoss: 0.002144\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.001877\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002755\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.001708\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.001640\n",
      "Epoch: 19/20. Train set: Average loss: 0.0020\n",
      "Train: [0/60000 (0%)]\tLoss: 0.005612\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.001976\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002571\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002789\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002744\n",
      "Epoch: 20/20. Train set: Average loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "fit(triplet_train_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbbd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f2d5b4c",
   "metadata": {},
   "source": [
    "# VALIDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6173dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"Models/tripletMNIST.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69d4ecdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_loaded = torch.load(\"Models/tripletMNIST.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b25f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, triplet_test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for (anchor, positive, negative),[] in triplet_test_loader:\n",
    "            anchor_embedding,positive_embedding,negative_embedding = model(anchor,positive,negative)\n",
    "            positive_distance = F.pairwise_distance(anchor_embedding, positive_embedding)\n",
    "            negative_distance = F.pairwise_distance(anchor_embedding, negative_embedding)\n",
    "            correct += torch.sum(positive_distance < negative_distance).item()\n",
    "            total += anchor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy : {:.2f}%\\nTime : {:.2f} SECONDS'.format(accuracy * 100,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c20e6096",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.67%\n",
      "Time : 7.93 SECONDS\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_loaded,triplet_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa20c7",
   "metadata": {},
   "source": [
    "# WITH FACEBOOK AI SIMILARITY SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a7727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, **kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b5127cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa5481e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs1 = None\n",
    "labels1 = []\n",
    "for idx,i in enumerate(train_loader):\n",
    "    if idx==10000: break\n",
    "    I, L = i\n",
    "    labels1.append(L)\n",
    "    emb = model_loaded(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if embs1 is None:\n",
    "        embs1 = emb\n",
    "    else:\n",
    "        embs1 = torch.cat((embs1, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a96e3d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs2 = None\n",
    "labels2 = []\n",
    "for i in test_loader:\n",
    "    I, L = i\n",
    "    labels2.append(L)\n",
    "    emb = model_loaded(I)\n",
    "    if embs2 is None:\n",
    "        embs2 = emb\n",
    "    else:\n",
    "        embs2 = torch.cat((embs2, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e318ad10",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = embs1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91ab53be",
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = faiss.IndexFlatL2(embs.shape[1])  # Assuming embs.shape[1] represents the dimensionality of the embeddings\n",
    "index1.add(embs)\n",
    "\n",
    "nlist = 100  # Number of cells/buckets\n",
    "quantizer = faiss.IndexFlatL2(embs.shape[1])  # Quantizer index (same as IndexFlatL2)\n",
    "index2 = faiss.IndexIVFFlat(quantizer, embs.shape[1], nlist)\n",
    "index2.train(embs)\n",
    "index2.add(embs)\n",
    "\n",
    "index3 = faiss.IndexHNSWFlat(embs.shape[1], 32)  # M = 32 for the HNSW index\n",
    "index3.add(embs)\n",
    "\n",
    "nbits = 8  # Number of bits for the LSH hash\n",
    "index4 = faiss.IndexLSH(embs.shape[1], nbits)\n",
    "index4.add(embs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6bb1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fba5df4e",
   "metadata": {},
   "source": [
    "# EVALUATE WITH FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c1a72fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluatewithfaiss(embs,index):\n",
    "    TOTAL = len(embs)\n",
    "    CORRECT = 0\n",
    "    start = time.time()\n",
    "    for idx,emb in enumerate(embs):\n",
    "        label = index.search(emb.detach().reshape(1,-1),1)[1][0][0]\n",
    "        CORRECT += labels1[label]==labels2[idx]\n",
    "    return (CORRECT/TOTAL*100).item(),f'{time.time()-start} SECONDS'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0033c74e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexFlatL2 : (98.8499984741211, '0.8810124397277832 SECONDS')\n",
      "IndexIVFFlat : (98.81999969482422, '0.41861629486083984 SECONDS')\n",
      "IndexHNSWFlat : (98.8499984741211, '0.5943887233734131 SECONDS')\n",
      "IndexLSH : (87.05000305175781, '0.948662281036377 SECONDS')\n"
     ]
    }
   ],
   "source": [
    "print(f'IndexFlatL2 : {evaluatewithfaiss(embs2,index1)}')\n",
    "print(f'IndexIVFFlat : {evaluatewithfaiss(embs2,index2)}')\n",
    "print(f'IndexHNSWFlat : {evaluatewithfaiss(embs2,index3)}')\n",
    "print(f'IndexLSH : {evaluatewithfaiss(embs2,index4)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
