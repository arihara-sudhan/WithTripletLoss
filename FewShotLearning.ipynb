{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "60608e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import faiss\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4eb484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = 0.1307, 0.3081\n",
    "\n",
    "train_dataset = MNIST('./Datasets/MNIST', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((mean,), (std,))\n",
    "                             ]))\n",
    "test_dataset = MNIST('./Datasets/MNIST', train=False, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean,), (std,))\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5762b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)  # .pow(.5)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  # .pow(.5)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeacde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletMNIST(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.train = self.mnist_dataset.train\n",
    "        self.transform = self.mnist_dataset.transform\n",
    "\n",
    "        if self.train:\n",
    "            self.train_labels = self.mnist_dataset.train_labels\n",
    "            self.train_data = self.mnist_dataset.train_data\n",
    "            self.labels_set = set(self.train_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.train_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "        else:\n",
    "            self.test_labels = self.mnist_dataset.test_labels\n",
    "            self.test_data = self.mnist_dataset.test_data\n",
    "            # generate fixed triplets for testing\n",
    "            self.labels_set = set(self.test_labels.numpy())\n",
    "            self.label_to_indices = {label: np.where(self.test_labels.numpy() == label)[0]\n",
    "                                     for label in self.labels_set}\n",
    "\n",
    "            random_state = np.random.RandomState(29)\n",
    "\n",
    "            triplets = [[i,\n",
    "                         random_state.choice(self.label_to_indices[self.test_labels[i].item()]),\n",
    "                         random_state.choice(self.label_to_indices[\n",
    "                                                 np.random.choice(\n",
    "                                                     list(self.labels_set - set([self.test_labels[i].item()]))\n",
    "                                                 )\n",
    "                                             ])\n",
    "                         ]\n",
    "                        for i in range(len(self.test_data))]\n",
    "            self.test_triplets = triplets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.train:\n",
    "            img1, label1 = self.train_data[index], self.train_labels[index].item()\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            negative_label = np.random.choice(list(self.labels_set - set([label1])))\n",
    "            negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "            img2 = self.train_data[positive_index]\n",
    "            img3 = self.train_data[negative_index]\n",
    "        else:\n",
    "            img1 = self.test_data[self.test_triplets[index][0]]\n",
    "            img2 = self.test_data[self.test_triplets[index][1]]\n",
    "            img3 = self.test_data[self.test_triplets[index][2]]\n",
    "\n",
    "        img1 = Image.fromarray(img1.numpy(), mode='L')\n",
    "        img2 = Image.fromarray(img2.numpy(), mode='L')\n",
    "        img3 = Image.fromarray(img3.numpy(), mode='L')\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        return (img1, img2, img3), []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134146b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.convnet = nn.Sequential(nn.Conv2d(1, 32, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2),\n",
    "                                     nn.Conv2d(32, 64, 5), nn.PReLU(),\n",
    "                                     nn.MaxPool2d(2, stride=2))\n",
    "\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 4 * 4, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 256),\n",
    "                                nn.PReLU(),\n",
    "                                nn.Linear(256, 2)\n",
    "                                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.convnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7324977",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNet(nn.Module):\n",
    "    def __init__(self, embedding_net):\n",
    "        super(TripletNet, self).__init__()\n",
    "        self.embedding_net = embedding_net\n",
    "\n",
    "    def forward(self, x1, x2=None, x3=None):\n",
    "        if x2 is None and x3 is None:\n",
    "            return self.embedding_net(x1)\n",
    "        return self.embedding_net(x1),self.embedding_net(x2),self.embedding_net(x3)\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.embedding_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c04d93ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zadmin/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:65: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n",
      "/home/zadmin/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:75: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "/home/zadmin/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n",
      "/home/zadmin/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "triplet_train_dataset = TripletMNIST(train_dataset) \n",
    "triplet_test_dataset = TripletMNIST(test_dataset)\n",
    "batch_size = 128\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
    "triplet_train_loader = torch.utils.data.DataLoader(triplet_train_dataset, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "triplet_test_loader = torch.utils.data.DataLoader(triplet_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "margin = 1.\n",
    "embedding_net = EmbeddingNet()\n",
    "model = TripletNet(embedding_net)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "loss_fn = TripletLoss(margin)\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, 8, gamma=0.1, last_epoch=-1)\n",
    "n_epochs = 20\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be26f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval, metrics=[],\n",
    "        start_epoch=0):\n",
    "    for epoch in range(0, start_epoch):\n",
    "        scheduler.step()\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        scheduler.step()\n",
    "        train_loss, metrics = train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics)\n",
    "        message = 'Epoch: {}/{}. Train set: Average loss: {:.4f}'.format(epoch + 1, n_epochs, train_loss)\n",
    "        for metric in metrics:\n",
    "            message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def train_epoch(train_loader, model, loss_fn, optimizer, cuda, log_interval, metrics):\n",
    "    for metric in metrics:\n",
    "        metric.reset()\n",
    "\n",
    "    model.train()\n",
    "    losses = []\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target if len(target) > 0 else None\n",
    "        if not type(data) in (tuple, list):\n",
    "            data = (data,)\n",
    "        if cuda:\n",
    "            data = tuple(d.cuda() for d in data)\n",
    "            if target is not None:\n",
    "                target = target.cuda()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(*data)\n",
    "\n",
    "        if type(outputs) not in (tuple, list):\n",
    "            outputs = (outputs,)\n",
    "\n",
    "        loss_inputs = outputs\n",
    "        if target is not None:\n",
    "            target = (target,)\n",
    "            loss_inputs += target\n",
    "\n",
    "        loss_outputs = loss_fn(*loss_inputs)\n",
    "        loss = loss_outputs[0] if type(loss_outputs) in (tuple, list) else loss_outputs\n",
    "        losses.append(loss.item())\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric(outputs, target, loss_outputs)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            message = 'Train: [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                batch_idx * len(data[0]), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), np.mean(losses))\n",
    "            for metric in metrics:\n",
    "                message += '\\t{}: {}'.format(metric.name(), metric.value())\n",
    "\n",
    "            print(message)\n",
    "            losses = []\n",
    "\n",
    "    total_loss /= (batch_idx + 1)\n",
    "    return total_loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a4b1f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0/60000 (0%)]\tLoss: 0.999841\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.286063\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.130241\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.099396\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.075484\n",
      "Epoch: 1/20. Train set: Average loss: 0.1385\n",
      "Train: [0/60000 (0%)]\tLoss: 0.032791\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.070450\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.053684\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.046154\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.041406\n",
      "Epoch: 2/20. Train set: Average loss: 0.0510\n",
      "Train: [0/60000 (0%)]\tLoss: 0.044681\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.032559\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.033993\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.035605\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.032804\n",
      "Epoch: 3/20. Train set: Average loss: 0.0339\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000803\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.033608\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.028485\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.023640\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.022749\n",
      "Epoch: 4/20. Train set: Average loss: 0.0266\n",
      "Train: [0/60000 (0%)]\tLoss: 0.058188\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.020101\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.019652\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.023065\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.016663\n",
      "Epoch: 5/20. Train set: Average loss: 0.0199\n",
      "Train: [0/60000 (0%)]\tLoss: 0.019992\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.023207\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.018909\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.019042\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.021654\n",
      "Epoch: 6/20. Train set: Average loss: 0.0209\n",
      "Train: [0/60000 (0%)]\tLoss: 0.013859\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.015479\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.017354\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.018574\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.017835\n",
      "Epoch: 7/20. Train set: Average loss: 0.0169\n",
      "Train: [0/60000 (0%)]\tLoss: 0.018489\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.010421\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.007147\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.006884\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.008354\n",
      "Epoch: 8/20. Train set: Average loss: 0.0082\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000584\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.006473\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.007242\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.005647\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.006210\n",
      "Epoch: 9/20. Train set: Average loss: 0.0061\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.004252\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003791\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.005221\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.005832\n",
      "Epoch: 10/20. Train set: Average loss: 0.0047\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003710\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.004716\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.004937\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.005217\n",
      "Epoch: 11/20. Train set: Average loss: 0.0044\n",
      "Train: [0/60000 (0%)]\tLoss: 0.008102\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.004598\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.005636\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003343\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003035\n",
      "Epoch: 12/20. Train set: Average loss: 0.0042\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003616\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.004022\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.004927\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003443\n",
      "Epoch: 13/20. Train set: Average loss: 0.0038\n",
      "Train: [0/60000 (0%)]\tLoss: 0.012870\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003200\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003630\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003735\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003126\n",
      "Epoch: 14/20. Train set: Average loss: 0.0035\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003111\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003527\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.003305\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003231\n",
      "Epoch: 15/20. Train set: Average loss: 0.0033\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.003415\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.001840\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002885\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002928\n",
      "Epoch: 16/20. Train set: Average loss: 0.0028\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000374\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.002198\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003011\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002421\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002478\n",
      "Epoch: 17/20. Train set: Average loss: 0.0025\n",
      "Train: [0/60000 (0%)]\tLoss: 0.008990\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.001707\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002489\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002486\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.003232\n",
      "Epoch: 18/20. Train set: Average loss: 0.0024\n",
      "Train: [0/60000 (0%)]\tLoss: 0.000000\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.002472\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.003052\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.002157\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002591\n",
      "Epoch: 19/20. Train set: Average loss: 0.0025\n",
      "Train: [0/60000 (0%)]\tLoss: 0.012773\n",
      "Train: [12800/60000 (21%)]\tLoss: 0.002919\n",
      "Train: [25600/60000 (43%)]\tLoss: 0.002552\n",
      "Train: [38400/60000 (64%)]\tLoss: 0.001710\n",
      "Train: [51200/60000 (85%)]\tLoss: 0.002240\n",
      "Epoch: 20/20. Train set: Average loss: 0.0024\n"
     ]
    }
   ],
   "source": [
    "fit(triplet_train_loader, model, loss_fn, optimizer, scheduler, n_epochs, cuda, log_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcbbd44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6173dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,\"tripletMNIST.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69d4ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = torch.load(\"tripletMNIST.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "8b25f944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def evaluate_model(model, triplet_test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        start = time.time()\n",
    "        for (anchor, positive, negative),[] in triplet_test_loader:\n",
    "            anchor_embedding,positive_embedding,negative_embedding = model(anchor,positive,negative)\n",
    "            positive_distance = F.pairwise_distance(anchor_embedding, positive_embedding)\n",
    "            negative_distance = F.pairwise_distance(anchor_embedding, negative_embedding)\n",
    "            correct += torch.sum(positive_distance < negative_distance).item()\n",
    "            total += anchor.size(0)\n",
    "    accuracy = correct / total\n",
    "    print('Accuracy : {:.2f}%\\nTime : {:.2f} SECONDS'.format(accuracy * 100,time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c20e6096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 99.55%\n",
      "Time : 8.49 SECONDS\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model_loaded,triplet_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "89a7727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=True, **kwargs)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "219821c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "embs1 = []\n",
    "for data in test_loader:\n",
    "    embs1.append(model_loaded(data[0]).detach())\n",
    "print(len(embs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "fa5481e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs1 = None\n",
    "labels1 = []\n",
    "for idx,i in enumerate(train_loader):\n",
    "    if idx==10000: break\n",
    "    I, L = i\n",
    "    labels1.append(L)\n",
    "    emb = model_loaded(I) # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    emb = emb.detach()\n",
    "    if embs1 is None:\n",
    "        embs1 = emb\n",
    "    else:\n",
    "        embs1 = torch.cat((embs1, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f361ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4a96e3d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embs2 = None\n",
    "labels2 = []\n",
    "for i in test_loader:\n",
    "    I, L = i\n",
    "    labels2.append(L)\n",
    "    emb = model_loaded(I)  # Assuming `model_loaded(I)` returns a PyTorch tensor\n",
    "    if embs2 is None:\n",
    "        embs2 = emb\n",
    "    else:\n",
    "        embs2 = torch.cat((embs2, emb), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5d834aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vector_dimension = embs1_np.shape[1]\n",
    "index = faiss.IndexFlatL2(vector_dimension)\n",
    "index.add(embs1.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "a68a46b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "index = faiss.IndexFlatL2(embs1.shape[1]) \n",
    "index.add(embs1.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "1e910fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "D,I = index.search(embs1[0].detach().reshape(1,-1),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad5bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1c1a72fa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def evaluatewithfaiss(embs):\n",
    "    TOTAL = len(embs)\n",
    "    CORRECT = 0\n",
    "    start = time.time()\n",
    "    for idx,emb in enumerate(embs):\n",
    "        label = index.search(emb.detach().reshape(1,-1),1)[1][0][0]\n",
    "        CORRECT += labels1[label]==labels2[idx]\n",
    "    return (CORRECT/TOTAL*100).item(),f'{time.time()-start} SECONDS'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0033c74e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98.77999877929688, '2.0873653888702393 SECONDS')"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluatewithfaiss(embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655689d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
